{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/gustavohroos/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/gustavohroos/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/gustavohroos/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/gustavohroos/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import re\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados = pd.read_csv('Lorde.tsv', sep='\\t')\n",
    "dados_limpos = dados.drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Corpus</th>\n",
       "      <th>Texto_Preparado</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I'm a liability I'm a liability Much for me Yo...</td>\n",
       "      <td>[im, liability, im, liability, much, youre, li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>You asked if I was feeling it, I’m psycho high...</td>\n",
       "      <td>[ask, feel, im, psycho, high, know, wont, reme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(Go back and tell it) Please could you be tend...</td>\n",
       "      <td>[go, back, tell, please, could, tender, sit, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Night, midnight, lose my mind Night, midnight,...</td>\n",
       "      <td>[night, midnight, lose, mind, night, midnight,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In my head, I play a supercut of us All the ma...</td>\n",
       "      <td>[head, play, supercut, u, magic, give, love, l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Corpus  \\\n",
       "0  I'm a liability I'm a liability Much for me Yo...   \n",
       "1  You asked if I was feeling it, I’m psycho high...   \n",
       "2  (Go back and tell it) Please could you be tend...   \n",
       "3  Night, midnight, lose my mind Night, midnight,...   \n",
       "4  In my head, I play a supercut of us All the ma...   \n",
       "\n",
       "                                     Texto_Preparado  \n",
       "0  [im, liability, im, liability, much, youre, li...  \n",
       "1  [ask, feel, im, psycho, high, know, wont, reme...  \n",
       "2  [go, back, tell, please, could, tender, sit, c...  \n",
       "3  [night, midnight, lose, mind, night, midnight,...  \n",
       "4  [head, play, supercut, u, magic, give, love, l...  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pré-processamento\n",
    "def preparar_texto(texto):\n",
    "    texto = re.sub(r'[^a-zA-Z\\s]', '', texto).lower()\n",
    "    \n",
    "    tokens = word_tokenize(texto)\n",
    "    \n",
    "    tokens_limpos = [palavra for palavra in tokens if palavra not in stopwords.words('english')]\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    def obter_pos_tag(word):\n",
    "        tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "        tag_dict = {\"J\": wordnet.ADJ,\n",
    "                    \"N\": wordnet.NOUN,\n",
    "                    \"V\": wordnet.VERB,\n",
    "                    \"R\": wordnet.ADV}\n",
    "        return tag_dict.get(tag, wordnet.NOUN)\n",
    "    \n",
    "    tokens_lematizados = [lemmatizer.lemmatize(w, obter_pos_tag(w)) for w in tokens_limpos]\n",
    "    \n",
    "    return tokens_lematizados\n",
    "\n",
    "dados_limpos['Texto_Preparado'] = dados_limpos['Corpus'].apply(preparar_texto)\n",
    "dados_limpos[['Corpus', 'Texto_Preparado']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentencas = dados_limpos['Texto_Preparado'].tolist()\n",
    "\n",
    "# Treinando o modelo\n",
    "\n",
    "modelo_word2vec = Word2Vec(sentences=sentencas, vector_size=100, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('though', 0.3209531307220459),\n",
       " ('different', 0.30907854437828064),\n",
       " ('psycho', 0.2959253489971161),\n",
       " ('well', 0.29396384954452515),\n",
       " ('tie', 0.286493718624115),\n",
       " ('send', 0.2776780128479004),\n",
       " ('cadillacs', 0.27569177746772766),\n",
       " ('perfect', 0.26952219009399414),\n",
       " ('else', 0.26385262608528137),\n",
       " ('grocery', 0.2627700865268707)]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mostrando as palavras mais similares a palavra feeling\n",
    "palavras_similares = modelo_word2vec.wv.most_similar('feeling', topn=10)\n",
    "\n",
    "palavras_similares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00673199, -0.00689098, -0.00925254,  0.00349097, -0.00167444,\n",
       "        0.00747178, -0.00784034,  0.00770493,  0.00914396,  0.00296698,\n",
       "       -0.00655855,  0.00407521,  0.00661385, -0.007174  ,  0.00433556,\n",
       "       -0.00256349,  0.00234737, -0.01017085,  0.00057437, -0.00867194,\n",
       "       -0.00627812, -0.00465405, -0.00872754, -0.01012931, -0.00670051,\n",
       "       -0.00346477,  0.00072443, -0.01140289, -0.00417207, -0.00471279,\n",
       "        0.00622272,  0.00494807, -0.00350561,  0.00790052, -0.00857536,\n",
       "        0.01076558, -0.00182819, -0.00069331,  0.0011879 ,  0.00431898,\n",
       "       -0.00390576, -0.00238252,  0.00453693,  0.0005084 ,  0.0015834 ,\n",
       "        0.00868869, -0.00955313, -0.00628194, -0.00525934, -0.00735084,\n",
       "       -0.00642885, -0.00161124, -0.00074687,  0.00646724,  0.00832964,\n",
       "        0.00058503,  0.00128625, -0.00871854,  0.0015272 , -0.00615092,\n",
       "        0.00693059,  0.00099018,  0.00483957, -0.00736168,  0.00120669,\n",
       "       -0.00879177, -0.00717424, -0.0009309 , -0.00727994, -0.0044623 ,\n",
       "       -0.00711602,  0.00997359,  0.00209279, -0.00203098, -0.00176932,\n",
       "       -0.0008814 , -0.00168172, -0.00510889, -0.00630814, -0.00775758,\n",
       "        0.00730702,  0.0047759 , -0.0080288 ,  0.00222778,  0.00212695,\n",
       "        0.00981948, -0.00236701,  0.01097969,  0.00929586, -0.00451067,\n",
       "       -0.00662887,  0.01099783,  0.003699  ,  0.00547397, -0.00183113,\n",
       "        0.01144936, -0.00158713, -0.00789169,  0.0085317 ,  0.00911163],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vetor da palavra feeling\n",
    "vector = modelo_word2vec.wv['feeling']\n",
    "\n",
    "vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.65033346"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculando a similaridade entre duas palavras\n",
    "similaridade = modelo_word2vec.wv.similarity('love', 'heart')\n",
    "\n",
    "# Mostrando a similaridade entre \"love\" e \"heart\"\n",
    "similaridade\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
